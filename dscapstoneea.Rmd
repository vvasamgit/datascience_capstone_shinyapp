---
title: "Data Science Capstone Exploratory DataAnalysis"
author: "Venkat Vasam"
date: "February 2nd, 2020"
output: html_document
---

```{r setup, include=FALSE,echo = TRUE, warning=FALSE,message=FALSE,comment=NA}
knitr::opts_chunk$set(echo = TRUE,cache=TRUE)
```


<!-- 

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 1. Demonstrate that you've downloaded the data and have successfully loaded it in.2. Create a basic report of summary statistics about the data sets.3. Report any interesting findings that you amassed so far.4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

Review criterialess 
Does the link lead to an HTML page describing the exploratory analysis of the training data set?
Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?
Has the data scientist made basic plots, such as histograms to illustrate features of the data?
Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?

-->

## Introduction

The goal of this effort is to understand the problem of how to predict next word using the given text documents.  After understanding the problem data should acquired and cleansed to  analyze text data and perform natural language processing steps such as creating corpus of text documents ,tokenizing, stemming, or forming N-grams using techniques of data science such as exploratory analysis and building predictive text models for prediction next word using the SwiftKey data set that has text documents in 4 languages.  

The English language text documents are used for performing exploratory analysis. The collection of text documents is called corpus. The exploratory analysis on corpus of SwiftKey text documents will clean the text data and detect the structure of text data, derive total number of lines, words and how words are used next to each other.
 


##Source Data Set 

The source data set is downloaded from 

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

##Development Tools

The R programming language packages are installed for the exploratory analysis effort using the R commands listed below. The quanteda package also needs RTools software for Windows platform. 


```{r echo=TRUE, eval=FALSE}
if (!require("devtools")) {
  install.packages("devtools")
  library(devtools)
}

if (!require("RColorBrewer")) {
  install.packages("RColorBrewer")
  library(RColorBrewer)
}

if (!require("quanteda")) {
  install.packages("quanteda")
  library(quanteda)
}

if (!require("data.table")) {
  install.packages("data.table")
  library(data.table)
}

if (!require("readr")) {
  install.packages("readr")
  library(readr)
}

if (!require("stringr")) {
  install.packages("stringr")
  library(readr)
}

if (!require("readtext")) {
  install.packages("readtext")
  library(readr)
}


if (!require("ggplot2")) {
  install.packages("ggplot2")
  library(readr)
}

if (!require("plotly")) {
  install.packages("plotly")
  library(plotly)
}

devtools::install_github("quanteda/quanteda.corpora")

devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)

devtools::install_github("kbenoit/quanteda.dictionaries")

```

The devtools package installs required  wc command for gathering file details. 

The read_lines from readr package will be used to read lines of text efficiently from the text files. 

The quanteda package provides libraries for natural languuage processing such as creating corpus of text documents,tokenizing, stemming, or forming ngrams. The readtext package is used to read files and create a corpus object.

The data.table is for processing text efficiently. The stringr package provides string manipulation with regular expressions. 



##Exploratory Data Analysis 

The following sections describe how to load data files , check the top and bottom of data, count "n"s and plot the details. Some of the common issues in the analysis of text data is large sizes of text documents, dealing with foreign characters and words and removing unwanted characters such a punctuaiton and stop workds.

###Define R Global Environment 

The text data/naural language processing ( NLP) involves dealing with large volumes of text so more computing rersources such as memory and CPU are needed. The NLP applications will be run in mobile phones where the amount of computing resources are limited so resource management functions are developed as shown below using R capabilities such as gc to force R collect unused memory by pefroming garbage collection.

  1. captureStartTime
  2. captureTimeTaken
  3. calcGc
  
```{r echo=TRUE, eval=FALSE}
#Define Global variables

fileSep<-.Platform$file.sep
fileInfo<-c()

gcDet<-data.table(memUsedMb=numeric(), maxMemUsedMb=numeric())
gcDet<-rbind(gcDet, list(0,0))
invisible(gc(reset=TRUE))
gcPrev<-invisible(gc())
gcNow<-invisible(gc())

#capture time function

captureStartTime<-function(){
  return(Sys.time())
}

#capture time taken function

captureTimeTaken<-function(pStartTime, msg=""){
  
  endTime <- Sys.time()
  
  timeTaken <- endTime - startTime

  print(paste0(msg, "time captured as: ",timeTaken,"secs"))
  
  return( timeTaken )
}  


# Capture Memory Usage function 
calcGc<-function(){
  gcPrev<-gcNow
  gcNow<-invisible(gc())
  gcDet<-data.table(memUsedMb=numeric(), maxMemUsedMb=numeric())
  gcDet<-rbind(gcDet, list(0,0))
  usedMb<-sum(gcNow[,2]-gcPrev[,2])
  maxUsedMb<-sum(gcNow[,6]-gcPrev[,6])
  gcDetRow<- c(usedMb,maxUsedMb)
  gcDet[1,]<-list(usedMb,maxUsedMb)
  print(paste0("Memory Used :",gcDet$memUsedMb, ", Max memory used:",gcDet$maxMemUsedMb))
  return (gcDet)
}

```
  

### Gather Text Corpus Summary Details

Instead of loading all the file data the WC command from RTools is used to collect file information such as line count and word count. The RTools should installed and PATH variable should be setup so that WC command can be run from R program.

```{r echo=TRUE, eval=FALSE}
#Replace the Rtools installation absolute path for wcHome
wcHome<-"C:/Instls/Rtools/bin" 
cmdWc<-paste0(wcHome, fileSep, "wc ")

#Run Operating System Command  function 
runCmd<- function(cmd, option, fileAbsPath){
  cmdRes<-system(paste(cmd,option,fileAbsPath,sep=" "),intern=TRUE) 
  cmdRes<-str_split(str_trim (cmdRes),"[[:space:]]+");
}

```


```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}

appStartTime <- Sys.time()

#Load libraries


if (!require("devtools")) {
  install.packages("devtools")
  library(devtools)
}

if (!require("RColorBrewer")) {
  install.packages("RColorBrewer")
  library(RColorBrewer)
}

if (!require("quanteda")) {
  install.packages("quanteda")
  library(quanteda)
}

if (!require("data.table")) {
  install.packages("data.table")
  library(data.table)
}

if (!require("readr")) {
  install.packages("readr")
  library(readr)
}

if (!require("stringr")) {
  install.packages("stringr")
  library(readr)
}

if (!require("readtext")) {
  install.packages("readtext")
  library(readr)
}


if (!require("ggplot2")) {
  install.packages("ggplot2")
  library(readr)
}

if (!require("plotly")) {
  install.packages("plotly")
  library(plotly)
}


#devtools::install_github("quanteda/quanteda.corpora")

#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)

#devtools::install_github("kbenoit/quanteda.dictionaries")




#Define Global variables

fileSep<-.Platform$file.sep
fileInfo<-c()

gcDet<-data.table(memUsedMb=numeric(), maxMemUsedMb=numeric())
gcDet<-rbind(gcDet, list(0,0))
invisible(gc(reset=TRUE))
gcPrev<-invisible(gc())
gcNow<-invisible(gc())

#capture time function

captureStartTime<-function(){
  return(Sys.time())
}

#capture time taken function

captureTimeTaken<-function(pStartTime, msg=""){
  
  endTime <- Sys.time()
  
  timeTaken <- endTime - startTime

  #print(paste0(msg, "time captured. Start Time :", pStartTime, ", End Time :",endTime, "Time Taken :",timeTaken))
  
  print(paste0(msg, " time captured as: ",timeTaken,"secs"))
  
  return( timeTaken )
}  


# Captire Memory Usage function 
calcGc<-function(){
  gcPrev<-gcNow
  gcNow<-invisible(gc())
  gcDet<-data.table(memUsedMb=numeric(), maxMemUsedMb=numeric())
  gcDet<-rbind(gcDet, list(0,0))
  usedMb<-sum(gcNow[,2]-gcPrev[,2])
  maxUsedMb<-sum(gcNow[,6]-gcPrev[,6])
  gcDetRow<- c(usedMb,maxUsedMb)
  gcDet[1,]<-list(usedMb,maxUsedMb)
  print(paste0("Memory Used :",gcDet$memUsedMb, ", Max memory used:",gcDet$maxMemUsedMb))
  return (gcDet)
}


#Gather file details such as name and sizes

gatherFileInfo<-function(pDataHomeDir,filePattern){
  
  oFileNames<-list.files(pDataHomeDir,pattern=filePattern)
  nFileNames<-sapply(oFileNames, function(x){ 
    gsub("(\\.)([a-zA-Z]*)(\\.)([a-zA-Z]*)$", "_\\2\\3\\4",x)
  }
  )
  
  curdir<-getwd()
  setwd(pDataHomeDir)
  
  file.rename(oFileNames,nFileNames)
  
  setwd(curdir)
  
  fileSep<-.Platform$file.sep
  
  absPath<-sapply(nFileNames, function(x){
    paste(pDataHomeDir,fileSep,x,sep="")
  })
  
  names(absPath)<-c()
  
  fileInfo<-file.info(absPath)
  
  rownames(fileInfo)<-c()
  
  fileName<-nFileNames
  
  names(fileName)<-c()
  
  names (absPath)<-c()
  
  fileInfo<-cbind(absPath, fileInfo,stringsAsFactors = FALSE )
  
  fileInfo<-cbind(fileName, fileInfo,stringsAsFactors = FALSE )
  
  return (setDT(fileInfo))
  
}

wcHome<-"C:/Instls/Rtools/bin" 
cmdWc<-paste0(wcHome, fileSep, "wc ")

#Run Operating System Command  function 
runCmd<- function(cmd, option, fileAbsPath){
  cmdRes<-system(paste(cmd,option,fileAbsPath,sep=" "),intern=TRUE) 
  cmdRes<-str_split(str_trim (cmdRes),"[[:space:]]+");
}

#Gather file counts function
gatherFileCounts<-function(pfileInfo){
  pfileInfo<-pfileInfo[,line_count:=.()]
  pfileInfo<-pfileInfo[,word_count:=.()]
  pfileInfo<-pfileInfo[,char_count:=.()]
  
  pfileCount<-nrow(pfileInfo)
  
  
  for (i in 1:pfileCount){
    #print(pfileInfo[i]$absPath)
    wcRes<-runCmd(cmdWc,"-lwm",pfileInfo[i]$absPath)
    pfileInfo[i]$line_count=wcRes[[1]][1]
    pfileInfo[i]$word_count=wcRes[[1]][2]
    pfileInfo[i]$char_count=wcRes[[1]][3]
  }
  return(pfileInfo)
}

createSampleDataFiles<-function (pSamplePercent, pFileInfo, pDataDirHome){
  
  #create sample percent directory under data diretcory
  
  subDir<-paste("sample",pSamplePercent, sep="_")
  
  
  dirExists<-ifelse(dir.exists(file.path(pDataDirHome, subDir)), TRUE, dir.create(file.path(pDataDirHome, subDir)))
  
  fileCount<-nrow(fileInfo)
  
  for(fi in 1:fileCount){
    # fi<-1
    
    #reading seperate lines by skipping causing problem so read all lines. The object can be removed after 
    # saving train and test data sets
    
    pFileAbsPath<-pFileInfo[fi]$absPath
    
    fAllData<-read_lines(pFileAbsPath)
    
    trnSampleFile<-paste("train","1",fileInfo[fi]$fileName,sep="_")
    trnSampleFile
    trnSampleFileAbs<-paste0(file.path(dataDirHome, subDir),fileSep,trnSampleFile )
    
    trnSampleFileAbs
    
    tstSampleFile<-paste("test","1",fileInfo[fi]$fileName,sep="_")
    tstSampleFile
    tstSampleFileAbs<-paste0(file.path(dataDirHome, subDir),fileSep,tstSampleFile )
    tstSampleFileAbs
    
    # Based on the total number of lines in file create sample line numbers for training and test sets out of 1 percent
    
    lineCount<-as.integer(fileInfo[fi]$line_count)
    
    pLineCount<-floor(lineCount*samplePercent/100)
    
    set.seed(20)
    
    sLineNos<-sample(lineCount, size=pLineCount, replace=FALSE)
    
    sTrnLineNos<-sample(sLineNos, size=0.8*length(sLineNos),replace=FALSE)
    
    plineCount<- length(sTrnLineNos)
    
    ftrnData<-character(plineCount)
    
    for (i in 1:plineCount){
      ftrnData[i]<- fAllData[i]
    }
    
    write_lines(ftrnData,trnSampleFileAbs)
    
    sTestLineNos<-setdiff(sLineNos,sTrnLineNos)
    
    ptstlineCount<- length(sTestLineNos)
    
    
    ftstData<-character(ptstlineCount)
    
    for (i in 1:ptstlineCount){
      ftstData[i]<- fAllData[i]
    }
    
    write_lines(ftstData,tstSampleFileAbs)
    rm(fAllData)
  }
     
    sampleDir<-paste(pDataDirHome,fileSep,subDir,sep="")
    
    return (sampleDir )
  
}  #End Create Sample


createNgramAndClean<-function(pCorpusTokens,nGramCnt) {
  
  ngramToken <- tokens_ngrams(pCorpusTokens,nGramCnt)
  
 # ngramTokenCln<-tokens(ngramToken,what="fasterword",remove_punct=TRUE,remove_numbers=TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
  
  ngramTokenCln<-tokens(ngramToken)
  
  #ngramTokenCln <- tokens_select(ngramTokenCln, pattern = stopwords('en'), selection = 'remove')
  
  return(ngramTokenCln)
}


createDfmStem<-function(pNgramTokenCln) {
  
  #docDfmTrn <- dfm(pNgramTokenCln,tolower = TRUE, stem=TRUE, remove=stopwords('en'))
  
  docDfmTrn <- dfm(pNgramTokenCln,tolower = TRUE, stem=TRUE)
  
  
}


createNgramTopFeatBarPlot<-function(pDocDfmTrn, pTopN,pVarName) {

  docDfmSortTrn1<-dfm_sort(pDocDfmTrn,margin = "both")
  
  topFeatures <- topfeatures(docDfmSortTrn1, n=pTopN)
  
  topFeaturesDT <- data.table(nGramWordCount=topFeatures)
  
  topFeaturesDT<-topFeaturesDT[,nGramWords:=.(names(topFeatures))]
  
  topFeaturesDT
 
  topFeaturesPlot <- ggplot(topFeaturesDT, aes(x=reorder(nGramWords, -nGramWordCount), y=nGramWordCount))
  topFeaturesPlot <- topFeaturesPlot + geom_bar(position = "identity", stat = "identity")
  topFeaturesPlot <- topFeaturesPlot + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(paste0("Top ",pVarName, " Words")) + ylab("Count")
  
  print(topFeaturesPlot)
  
  return(topFeaturesPlot)
}
  

createNgramAndCleanDfm<-function(pCorpus,nGramCnt,pVarName) {
  
  docTokensTrn0 <- tokens_ngrams(pCorpus,nGramCnt)
  
  docTokensTrn1<-tokens(docTokensTrn0,remove_punct=TRUE,remove_numbers=TRUE)
  
  docTokensTrn1<-tokens(docTokensTrn1,remove_symbols = TRUE,remove_hyphens=TRUE)
  
  docTokensTrn1 <- tokens_select(docTokensTrn1, pattern = stopwords('en'), selection = 'remove')
  
  docDfmTrn <- dfm(docTokensTrn1,tolower = TRUE, stem=TRUE, remove=stopwords('en'))
  
  docDfmSortTrn1<-dfm_sort(docDfmTrn,margin = "both")
  
  textplot_wordcloud (docDfmSortTrn1, max_words=100,color = brewer.pal(8,"Accent"))
  
  topFeatures <- topfeatures(docDfmSortTrn1, n=20)
  
  topFeaturesDf <- data.frame(topFeatures)
  
  topFeaturesDf["nGram"] <- rownames(topFeaturesDf)
  
  topFeaturesPlot <- ggplot(topFeaturesDf, aes(x=reorder(nGram, -topFeatures), y=topFeatures))
  topFeaturesPlot <- topFeaturesPlot + geom_bar(position = "identity", stat = "identity")
  topFeaturesPlot <- topFeaturesPlot + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(paste0("Top ",pVarName, " Word")) + ylab("Count")
  
  print(topFeaturesPlot)
  
  return(docDfmSortTrn1)
  
}


#Start Execution of application


curdir<-getwd()

dataURL<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

dataDirHome<-"./data"

dataZipFilePath<- paste0(dataDirHome, "/tmp/Coursera-SwiftKey.zip")

dataZipExtractDir<-paste0(dataDirHome, "/orig")

#Download data zip file and extract

if(!file.exists(dataZipFilePath)){
 download.file(dataURL, dataZipFilePath)
 #Extract zip file 
 unzip(dataZipFilePath,exdir=dataZipExtractDir)
}

dataDir<-paste0(dataZipExtractDir,"/final/en_US")


fileInfo<-gatherFileInfo(dataDir,"*.txt$")

fileInfo<-gatherFileCounts(fileInfo)

fileCounts<-fileInfo[,.(fileName,size,line_count,word_count,char_count)]

fileCounts

fileCountsMelt<-melt(fileCounts, id.vars=c("fileName"), measure.vars = c("line_count","word_count"))


fileCountsMelt<-fileCountsMelt[,value_millions:=as.integer(value)/1000000]


filePlot<-ggplot(fileCountsMelt, aes(x = fileName, y = value_millions, fill = variable))+
  geom_col(position = "dodge")+
  coord_cartesian( ylim=c(1, 50))+
xlab("SwiftKey Corpus English Files")+
  ylab("Value in millions")

ggplotly(filePlot)

```


### Create Sample Data From Entire Source Data Set

The following shows summary details of 1 percent of the sample data set. The text lines from the files are choosen by sampling the line numbers. The sample data files are saved to a sample directory so that later quanteda corpus can be created by reading all the sample files using readtext function. The file names are created with docvars for dataset_category such as train and test, sample_percent,locale(langauage) , country, content_catgeory such as blogs, news and twitter. The sample corpus will be used to create tokens and saved to file system for later use. 

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}

samplePercent<-1

startTime <- captureStartTime()

sampleDir<-createSampleDataFiles(samplePercent, fileInfo, dataDirHome)

sampleTime<-captureTimeTaken(startTime, paste0(samplePercent," percent sample data files creation "))

gcDet<-calcGc()

```

### Create corpus and N-grams

The following shows details of unigram, bigram and trigrams.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}

startTime <- captureStartTime()

docDf <- readtext(paste(sampleDir,"/*.txt", sep=""),
                    docvarsfrom = "filenames",
                    docvarnames = c("dataset_category", "sample_percent","locale","country", "content_catgeory"),
                    dvsep = "_",
                    encoding = "UTF-8")

docCorpus <- corpus(docDf)

#Create trainig corpus object

docCorpTrn <- corpus_subset(docCorpus, dataset_category=="train")

#Create testing corpus object

docCorpTst <- corpus_subset(docCorpus, dataset_category=="test")



docTokensTrn <- tokens(docCorpTrn,what="fastestword",
                       remove_punct=TRUE,remove_numbers=TRUE,
                       remove_symbols = TRUE,remove_hyphens=TRUE,
                       remove_separators = TRUE,
                       remove_url = TRUE)

timeTaken <- captureTimeTaken(startTime, "Corpus (training and test) and tokens creation")

gcDet<-calcGc()
                   

```

The following shows the word cloud of unigram words.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}


startTime <- captureStartTime()

nGramVarName<-"Unigram"

#docNgram1Dfm<-createNgramAndCleanDfm (docTokensTrn,1, nGramVarName)

docTokensTrnCln<-createNgramAndClean(docTokensTrn,1)
docDfmrn1<-createDfmStem(docTokensTrnCln)
textplot_wordcloud (docDfmrn1, max_words=100,color = brewer.pal(8,"Accent"))

timeTaken <- captureTimeTaken(startTime, "Unigram and word cloud creation")

gcDet<-calcGc()
```


The following shows the bar graph of top unigram words.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}


startTime <- captureStartTime()

unigramPlot<-createNgramTopFeatBarPlot(docDfmrn1, 20,nGramVarName)

timeTaken <- captureTimeTaken(startTime, "Unigram plot creation")

gcDet<-calcGc()

```


The following shows the word cloud of bigram words.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}


startTime <- captureStartTime()

#docNgram2Dfm<-createNgramAndCleanDfm (docTokensTrn,2, "Bigram")

docTokensTrncln2<-createNgramAndClean(docTokensTrn,2)

docDfmrn2<-createDfmStem(docTokensTrncln2)

textplot_wordcloud (docDfmrn2, max_words=100,color = brewer.pal(8,"Accent"))

timeTaken <- captureTimeTaken(startTime, "bigram and word cloud creation")
gcDet<-calcGc()

```


The following shows the bar graph of top bigram words.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}


startTime <- captureStartTime()

bigramPlot<-createNgramTopFeatBarPlot(docDfmrn2, 20, "Bigram")

timeTaken <- captureTimeTaken(startTime, "bigram plot creation")

gcDet<-calcGc()

```


The following shows the word cloud of trigram words.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}


startTime <- captureStartTime()

#docNgram3Dfm<-createNgramAndCleanDfm (docTokensTrn,3, "trigram")

docTokensTrncln3<-createNgramAndClean(docTokensTrn,3)

docDfmrn3<-createDfmStem(docTokensTrncln3)

textplot_wordcloud (docDfmrn3, max_words=100,color = brewer.pal(8,"Accent"))


timeTaken <- captureTimeTaken(startTime, "trigram and word cloud creation")
gcDet<-calcGc()


```


The following shows the bar graph of top trigram words.

```{r echo=FALSE, eval=TRUE,warning=FALSE,message=FALSE,comment=NA,cache=TRUE}

startTime <- captureStartTime()
trigramPlot<-createNgramTopFeatBarPlot(docDfmrn3, 20, "Trigram")

timeTaken <- captureTimeTaken(startTime, "trigram plot creation")

gcDet<-calcGc()


totaltimeTaken <- captureTimeTaken(appStartTime, "Total Sample Exploratory Analysis  ")


```

## Plan for creating a prediction algorithm and Shiny app

After the initial exploratory analysis, the sampling and token creation will be repeated so that file processing will be completed with less usage of memory and time. The token files can be reloaded later to create tokens for entire set of blogs, news and twitter data. 

The N-Grams can be used models for both supervised learning and unsupervised learning.  As mentioned in "The Elements of Statistical Learning" book the supervised machine learning has both outcome and feature variables for creating data models. In the unsupervised learning problem, there will be only the features and have no measurements of the outcome. So unsupervised learning deals with data classification and describes how the data are organized or clustered.  In regression models the quantitative outputs are predicted and in classification models the qualitative outputs are predicted.

As mentioned in the "Data Science Specialization Community Mentor Content Repository" by Len Greski, planning to create a data table using above n-grams. The data table will have three columns, first column is n-gram words except last word, second column is predicted last word of the n-gram, third column is count variable frequencies / probabilities of n-gram. As described in N-Grams about Markov assumption the data table with three columns will create Markov chains. The Markov probability models assume that the future unit can be predicted by not looking far in to past. The assumption that the probability of a word depends only on the previous word is called a Markov assumption. The quanteda textstat_frequency function provides both term and document frequencies.

The intuitive way to estimate the probabilities is the maximum likelihood estimation or MLE. The MLE estimate for the parameters of an N-gram model by getting counts from corpus and normalize the counts.  For example, to compute the probability of the bigram word, the total count of the bigram C(xy) is calculated and divided by the sum of all bigrams that share the first word x, which is the count of unigram C(x).
                                       P(y/x)  = C(xy)/C(x)

Since probabilities are between 0 and 1, multiplying N-grams will result in numerical underflow, so the log probabilities will be used to represent and compute language models.   The log probabilities can be added and compute exponent of the sum to get regular probability.

The models can be evaluated in two ways: Extrinsic and Intrinsic.   In the extrinsic evaluation, the language model is used in entire application and measured how much the application improves.  The intrinsic evaluation creates training and test data set.  The probabilities of N-gram model comes from training corpus and applied it on test set to measure quality of N-gram model.  The model that gives higher probability and predicts test set accurately is better model.  Both training and test sets should be from same genre and use common grammar.

The N-GRAMS chapter in Speech and Language Processing book mentions two approaches for dealing unknown words. The first approach is to create a closed vocabulary (word list) and use special pseudo word <UNK>. Convert any word from training set that is not there in vocabulary to <UNK>. Estimate the probabilities of <UNK> from its counts just like other regular words. The second approach is used when fixed vocabulary is not possible. In the second approach the first instance of every word in training data will be replaced by <UNK>.

The smoothing or discounting is used to handle zero probabilities of words that appear in unknown context in the test set. In smoothing some of the probability mass of the more frequent words are given to unknown events. Laplace or add -1 smoothing adds 1 to all bigram counts so that there are no zero bigram counts.  The counts are incremented by 1 so for calculating probabilities correctly the denominator count will be increased by V words in the vocabulary for getting correct probability distribution. If higher-order N grams are not discounted then probabilities assigned to all words will be greater than 1. 
Backoff and interpolation technique can be applied when there is no example of N-gram. In the backoff technique the probability will be estimated using N-1 Gram. In the interpolation the probability estimates from all N-gram estimators are weighed and combined.  The weight lambda is selected in such a way that the sum of weights is equal to 1.
Kneser-Ney discounting uses a fixed absolute discount value such as 0.75 and 0.5 for bigrams with count 1.
Stupid bakeoff is another technique where discounting not applied to higher order N-gram, instead simply backoff to a lower order N-gram, weighed by a fixed (context-independent) weight lambda such as 0.4. The back off terminates in unigram.
 

##Appendix

The resources given in the course material are used and listed below as reference



1. Text mining infrastucture in R
   https://www.jstatsoft.org/article/view/v025i05

2. CRAN Task View: Natural Language Processing
        https://cran.r-project.org/web/views/NaturalLanguageProcessing.html
        
3.Speech and Language Processing (3rd ed. draft)
  https://web.stanford.edu/~jurafsky/slp3/
  N-GRAMS
    https://web.stanford.edu/~jurafsky/slp3/3.pdf
    
4. Data Science Specialization Community Mentor Content Repository
  https://github.com/lgreski/datasciencectacontent
  
5. The Elements of Statistical Learning
https://web.stanford.edu/~hastie/ElemStatLearn/

  
    
  





